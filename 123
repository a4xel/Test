Date part
# date checking functions
def month_check(month):
    if month > 0 and month <= 12: ## If month is between 1 and 12, return True.
        return True
    else:
        return False

def day_check(month, day):
    days_in_month = {1:31, 2:28, 3:31, 4:30, 5:31, 6:30, 7:31, 8:31, 9:30, 10:31, 11:30, 12:31}
    if month > 0 and month <= 12 and 0 < day <= days_in_month[month]:
        return True
    else:
        return False

def year_check(year):
    if len(year) >= 1 and len(year) <= 4: ## Check if year has between 1 to 4 numbers and return True.
        return True
    else:
        return False

earliest = str("1990-01-01")

def start_date_check(start_date):
  if startdate > earliest:
    return True
  else: 
    return False

today = str("2021-04-30")

def end_date_check(start_date):
  if enddate < today:
    return True
  else: 
    return False

def start_before_end(startdate, enddate):
  if startdate < enddate:
    return True
  else: 
    return False

# ask start date and check if correct
startdate = None
while startdate is None:
  try: 
    startdate = str(input("Enter the start date in YYYY-MM-DD format: ")) 
    year,month,day = startdate.split("-") 
    month_validity = month_check(int(month)) 
    day_validity = day_check(int(month),int(day)) 
    year_validity = year_check(year)
    start_validity = start_date_check(startdate)

    if month_validity and day_validity and year_validity and start_validity:
      print("The date {0} is valid.".format(startdate))

    else:
      print("The date {0} is invalid.".format(startdate))
      startdate = None
      
  except ValueError:
    print('Your input was not valid. Please enter the date in YYYY-MM-DD format.')  
    startdate = None
     

#Ask for end date
enddate = None
while enddate is None:
  try: 
    enddate = str(input("Enter the end date in YYYY-MM-DD format: ")) 
    year,month,day = enddate.split("-") 
    month_validity = month_check(int(month)) 
    day_validity = day_check(int(month),int(day)) 
    year_validity = year_check(year)
    end_validity = end_date_check(enddate)
    start_end_validity = start_before_end(startdate, enddate)

    if month_validity and day_validity and year_validity and end_validity and start_end_validity : 
      print("The date {0} is valid.".format(enddate))

    else:
      print("The date {0} is invalid.".format(enddate))
      enddate = None

  except ValueError:
    print('Your input was not valid. Please enter the date in YYYY-MM-DD format.')
    enddate = None
  
    
### end date part












from datetime import datetime
import lxml
from lxml import html
import requests
import numpy as np
import pandas as pd


# Ask for the number of stocks the user wants to preselect
number = None
while number is None:
  try:
    number = int(input("Type the number of stocks you want to preselect: "))
  except ValueError:
    print ("You must type an integer, please try again.")


# Ask the user to input the tickers
tickers = []
i = 1
while i <= number: 
  try: 
    tick = str(input(f'Enter a {i}th Ticker: ')) 
    tick = tick.upper()
    tickers.append(tick)
    i += 1 
  except ValueError:
    print("Error - you have to enter a string. Try again.") 

# Print the ticker list
print("the ticker list is:", tickers )


def get_page(url):
    # Set up the request headers that we're going to use, to simulate
    # a request by the Chrome browser. Simulating a request from a browser
    # is generally good practice when building a scraper
    headers = {
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3',
        'Accept-Encoding': 'gzip, deflate, br',
        'Accept-Language': 'en-US,en;q=0.9',
        'Cache-Control': 'max-age=0',
        'Pragma': 'no-cache',
        'Referrer': 'https://google.com',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'
    }

    return requests.get(url, headers=headers)

def parse_rows(table_rows):
    parsed_rows = []

    for table_row in table_rows:
        parsed_row = []
        el = table_row.xpath("./div")

        none_count = 0

        for rs in el:
            try:
                (text,) = rs.xpath('.//span/text()[1]')
                parsed_row.append(text)
            except ValueError:
                parsed_row.append(np.NaN)
                none_count += 1

        if (none_count < 4):
            parsed_rows.append(parsed_row)
            
    return pd.DataFrame(parsed_rows)

def clean_data(df):
    df = df.set_index(0) # Set the index to the first column: 'Period Ending'.
    df = df.transpose() # Transpose the DataFrame, so that our header contains the account names
    
    # Rename the "Breakdown" column to "Date"
    cols = list(df.columns)
    cols[0] = 'Date'
    df = df.set_axis(cols, axis='columns', inplace=False)
    
    numeric_columns = list(df.columns)[1::] # Take all columns, except the first (which is the 'Date' column)

    for column_index in range(1, len(df.columns)): # Take all columns, except the first (which is the 'Date' column)
        df.iloc[:,column_index] = df.iloc[:,column_index].str.replace(',', '') # Remove the thousands separator
        df.iloc[:,column_index] = df.iloc[:,column_index].astype(np.float64) # Convert the column to float64
        
    return df

def scrape_table(url):
    # Fetch the page that we're going to parse
    page = get_page(url);

    # Parse the page with LXML, so that we can start doing some XPATH queries
    # to extract the data that we want
    tree = html.fromstring(page.content)

    # Fetch all div elements which have class 'D(tbr)'
    table_rows = tree.xpath("//div[contains(@class, 'D(tbr)')]")
    
    # Ensure that some table rows are found; if none are found, then it's possible
    # that Yahoo Finance has changed their page layout, or have detected
    # that you're scraping the page.
    assert len(table_rows) > 0
    
    df = parse_rows(table_rows)
    df = clean_data(df)
        
    return df

def scrape(symbol):
    print('Attempting to scrape data for ' + symbol)

    df_balance_sheet = scrape_table('https://finance.yahoo.com/quote/' + symbol + '/balance-sheet?p=' + symbol)
    df_balance_sheet = df_balance_sheet.set_index('Date')

    df_income_statement = scrape_table('https://finance.yahoo.com/quote/' + symbol + '/financials?p=' + symbol)
    df_income_statement = df_income_statement.set_index('Date')
    
    df_cash_flow = scrape_table('https://finance.yahoo.com/quote/' + symbol + '/cash-flow?p=' + symbol)
    df_cash_flow = df_cash_flow.set_index('Date')
    
    df_joined = df_balance_sheet \
        .join(df_income_statement, on='Date', how='outer', rsuffix=' - Income Statement') \
        .join(df_cash_flow, on='Date', how='outer', rsuffix=' - Cash Flow') \
        .dropna(axis=1, how='all') \
        .reset_index()
            
    df_joined.insert(1, 'Symbol', symbol) 
    
    return df_joined

def scrape_multi(tickers):
    return pd.concat([scrape(symbol) for symbol in tickers], sort=False)


df_combined = scrape_multi(tickers)

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)


df_combined
